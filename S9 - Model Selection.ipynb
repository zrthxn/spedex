{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c8feee-3861-40fd-8489-d0b840d4b3c5",
   "metadata": {},
   "source": [
    "# Seminar 9 - Model Selection & Regularization\n",
    "\n",
    "The task is to predict the fat content of a meat sample on the basis of its NIR absorbance spectrum. In real applications, the analysis of NIR-spectra is challenging and there is a wide range of analysis strategies.1 They typically involve principle component analysis (PCA) or make use of neural networks to infer chemical information (like fat content) from NIR-spectra. Here in this assignment it will do to apply multiple linear models with possibly some shrinkage involved.\n",
    "\n",
    "- Read in the data with the command readRDS and store the dataframe under the name nirData.\n",
    "- The data is formatted in «wide format». This means that each measurement has its own column: Check that you have indeed in total data of 215 meat samples with each a fat measurement (column fat) and 100 NIR-variables.\n",
    "- From the full data set, set aside a subset of 25% of all observations as test data set and use the remaining 75% as training data. We explain below why we will not use here a separate validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7af5f1c0-b5e0-4258-bcf5-19c020a5195f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 101</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>fat</th><th scope=col>NIR1</th><th scope=col>NIR2</th><th scope=col>NIR3</th><th scope=col>NIR4</th><th scope=col>NIR5</th><th scope=col>NIR6</th><th scope=col>NIR7</th><th scope=col>NIR8</th><th scope=col>NIR9</th><th scope=col>⋯</th><th scope=col>NIR91</th><th scope=col>NIR92</th><th scope=col>NIR93</th><th scope=col>NIR94</th><th scope=col>NIR95</th><th scope=col>NIR96</th><th scope=col>NIR97</th><th scope=col>NIR98</th><th scope=col>NIR99</th><th scope=col>NIR100</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>22.5</td><td>2.624678</td><td>2.617141</td><td>2.661152</td><td>2.632626</td><td>2.600933</td><td>2.640948</td><td>2.612343</td><td>2.615394</td><td>2.636674</td><td>⋯</td><td>3.003346</td><td>2.993943</td><td>2.957721</td><td>2.930983</td><td>2.917586</td><td>2.902895</td><td>2.878649</td><td>2.860067</td><td>2.816934</td><td>2.814325</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>40.1</td><td>2.831254</td><td>2.824333</td><td>2.852733</td><td>2.834804</td><td>2.875328</td><td>2.856527</td><td>2.871460</td><td>2.863564</td><td>2.863543</td><td>⋯</td><td>3.302150</td><td>3.321843</td><td>3.286252</td><td>3.255096</td><td>3.258644</td><td>3.242529</td><td>3.230499</td><td>3.214022</td><td>3.189640</td><td>3.125834</td></tr>\n",
       "\t<tr><th scope=row>3</th><td> 8.4</td><td>2.574347</td><td>2.604083</td><td>2.591397</td><td>2.590455</td><td>2.584382</td><td>2.580690</td><td>2.599903</td><td>2.588648</td><td>2.599094</td><td>⋯</td><td>2.709575</td><td>2.686483</td><td>2.676513</td><td>2.652291</td><td>2.613180</td><td>2.618521</td><td>2.572596</td><td>2.590596</td><td>2.558228</td><td>2.528773</td></tr>\n",
       "\t<tr><th scope=row>4</th><td> 5.9</td><td>2.808672</td><td>2.841908</td><td>2.838851</td><td>2.820005</td><td>2.823712</td><td>2.821007</td><td>2.849189</td><td>2.846039</td><td>2.839051</td><td>⋯</td><td>3.004082</td><td>2.977812</td><td>2.974285</td><td>2.916183</td><td>2.908807</td><td>2.874201</td><td>2.844503</td><td>2.863643</td><td>2.827721</td><td>2.795507</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>25.5</td><td>2.789264</td><td>2.791681</td><td>2.788367</td><td>2.790700</td><td>2.807376</td><td>2.787763</td><td>2.801650</td><td>2.815338</td><td>2.809073</td><td>⋯</td><td>3.312566</td><td>3.307170</td><td>3.300364</td><td>3.253781</td><td>3.245728</td><td>3.212378</td><td>3.196019</td><td>3.181573</td><td>3.152868</td><td>3.134712</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>42.7</td><td>3.015988</td><td>3.001191</td><td>3.029782</td><td>3.052708</td><td>3.034373</td><td>3.029786</td><td>3.047315</td><td>3.043427</td><td>3.061366</td><td>⋯</td><td>3.582822</td><td>3.562521</td><td>3.570639</td><td>3.533749</td><td>3.524914</td><td>3.534874</td><td>3.499294</td><td>3.484862</td><td>3.467219</td><td>3.446953</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 101\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & fat & NIR1 & NIR2 & NIR3 & NIR4 & NIR5 & NIR6 & NIR7 & NIR8 & NIR9 & ⋯ & NIR91 & NIR92 & NIR93 & NIR94 & NIR95 & NIR96 & NIR97 & NIR98 & NIR99 & NIR100\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 22.5 & 2.624678 & 2.617141 & 2.661152 & 2.632626 & 2.600933 & 2.640948 & 2.612343 & 2.615394 & 2.636674 & ⋯ & 3.003346 & 2.993943 & 2.957721 & 2.930983 & 2.917586 & 2.902895 & 2.878649 & 2.860067 & 2.816934 & 2.814325\\\\\n",
       "\t2 & 40.1 & 2.831254 & 2.824333 & 2.852733 & 2.834804 & 2.875328 & 2.856527 & 2.871460 & 2.863564 & 2.863543 & ⋯ & 3.302150 & 3.321843 & 3.286252 & 3.255096 & 3.258644 & 3.242529 & 3.230499 & 3.214022 & 3.189640 & 3.125834\\\\\n",
       "\t3 &  8.4 & 2.574347 & 2.604083 & 2.591397 & 2.590455 & 2.584382 & 2.580690 & 2.599903 & 2.588648 & 2.599094 & ⋯ & 2.709575 & 2.686483 & 2.676513 & 2.652291 & 2.613180 & 2.618521 & 2.572596 & 2.590596 & 2.558228 & 2.528773\\\\\n",
       "\t4 &  5.9 & 2.808672 & 2.841908 & 2.838851 & 2.820005 & 2.823712 & 2.821007 & 2.849189 & 2.846039 & 2.839051 & ⋯ & 3.004082 & 2.977812 & 2.974285 & 2.916183 & 2.908807 & 2.874201 & 2.844503 & 2.863643 & 2.827721 & 2.795507\\\\\n",
       "\t5 & 25.5 & 2.789264 & 2.791681 & 2.788367 & 2.790700 & 2.807376 & 2.787763 & 2.801650 & 2.815338 & 2.809073 & ⋯ & 3.312566 & 3.307170 & 3.300364 & 3.253781 & 3.245728 & 3.212378 & 3.196019 & 3.181573 & 3.152868 & 3.134712\\\\\n",
       "\t6 & 42.7 & 3.015988 & 3.001191 & 3.029782 & 3.052708 & 3.034373 & 3.029786 & 3.047315 & 3.043427 & 3.061366 & ⋯ & 3.582822 & 3.562521 & 3.570639 & 3.533749 & 3.524914 & 3.534874 & 3.499294 & 3.484862 & 3.467219 & 3.446953\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 101\n",
       "\n",
       "| <!--/--> | fat &lt;dbl&gt; | NIR1 &lt;dbl&gt; | NIR2 &lt;dbl&gt; | NIR3 &lt;dbl&gt; | NIR4 &lt;dbl&gt; | NIR5 &lt;dbl&gt; | NIR6 &lt;dbl&gt; | NIR7 &lt;dbl&gt; | NIR8 &lt;dbl&gt; | NIR9 &lt;dbl&gt; | ⋯ ⋯ | NIR91 &lt;dbl&gt; | NIR92 &lt;dbl&gt; | NIR93 &lt;dbl&gt; | NIR94 &lt;dbl&gt; | NIR95 &lt;dbl&gt; | NIR96 &lt;dbl&gt; | NIR97 &lt;dbl&gt; | NIR98 &lt;dbl&gt; | NIR99 &lt;dbl&gt; | NIR100 &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 22.5 | 2.624678 | 2.617141 | 2.661152 | 2.632626 | 2.600933 | 2.640948 | 2.612343 | 2.615394 | 2.636674 | ⋯ | 3.003346 | 2.993943 | 2.957721 | 2.930983 | 2.917586 | 2.902895 | 2.878649 | 2.860067 | 2.816934 | 2.814325 |\n",
       "| 2 | 40.1 | 2.831254 | 2.824333 | 2.852733 | 2.834804 | 2.875328 | 2.856527 | 2.871460 | 2.863564 | 2.863543 | ⋯ | 3.302150 | 3.321843 | 3.286252 | 3.255096 | 3.258644 | 3.242529 | 3.230499 | 3.214022 | 3.189640 | 3.125834 |\n",
       "| 3 |  8.4 | 2.574347 | 2.604083 | 2.591397 | 2.590455 | 2.584382 | 2.580690 | 2.599903 | 2.588648 | 2.599094 | ⋯ | 2.709575 | 2.686483 | 2.676513 | 2.652291 | 2.613180 | 2.618521 | 2.572596 | 2.590596 | 2.558228 | 2.528773 |\n",
       "| 4 |  5.9 | 2.808672 | 2.841908 | 2.838851 | 2.820005 | 2.823712 | 2.821007 | 2.849189 | 2.846039 | 2.839051 | ⋯ | 3.004082 | 2.977812 | 2.974285 | 2.916183 | 2.908807 | 2.874201 | 2.844503 | 2.863643 | 2.827721 | 2.795507 |\n",
       "| 5 | 25.5 | 2.789264 | 2.791681 | 2.788367 | 2.790700 | 2.807376 | 2.787763 | 2.801650 | 2.815338 | 2.809073 | ⋯ | 3.312566 | 3.307170 | 3.300364 | 3.253781 | 3.245728 | 3.212378 | 3.196019 | 3.181573 | 3.152868 | 3.134712 |\n",
       "| 6 | 42.7 | 3.015988 | 3.001191 | 3.029782 | 3.052708 | 3.034373 | 3.029786 | 3.047315 | 3.043427 | 3.061366 | ⋯ | 3.582822 | 3.562521 | 3.570639 | 3.533749 | 3.524914 | 3.534874 | 3.499294 | 3.484862 | 3.467219 | 3.446953 |\n",
       "\n"
      ],
      "text/plain": [
       "  fat  NIR1     NIR2     NIR3     NIR4     NIR5     NIR6     NIR7     NIR8    \n",
       "1 22.5 2.624678 2.617141 2.661152 2.632626 2.600933 2.640948 2.612343 2.615394\n",
       "2 40.1 2.831254 2.824333 2.852733 2.834804 2.875328 2.856527 2.871460 2.863564\n",
       "3  8.4 2.574347 2.604083 2.591397 2.590455 2.584382 2.580690 2.599903 2.588648\n",
       "4  5.9 2.808672 2.841908 2.838851 2.820005 2.823712 2.821007 2.849189 2.846039\n",
       "5 25.5 2.789264 2.791681 2.788367 2.790700 2.807376 2.787763 2.801650 2.815338\n",
       "6 42.7 3.015988 3.001191 3.029782 3.052708 3.034373 3.029786 3.047315 3.043427\n",
       "  NIR9     ⋯ NIR91    NIR92    NIR93    NIR94    NIR95    NIR96    NIR97   \n",
       "1 2.636674 ⋯ 3.003346 2.993943 2.957721 2.930983 2.917586 2.902895 2.878649\n",
       "2 2.863543 ⋯ 3.302150 3.321843 3.286252 3.255096 3.258644 3.242529 3.230499\n",
       "3 2.599094 ⋯ 2.709575 2.686483 2.676513 2.652291 2.613180 2.618521 2.572596\n",
       "4 2.839051 ⋯ 3.004082 2.977812 2.974285 2.916183 2.908807 2.874201 2.844503\n",
       "5 2.809073 ⋯ 3.312566 3.307170 3.300364 3.253781 3.245728 3.212378 3.196019\n",
       "6 3.061366 ⋯ 3.582822 3.562521 3.570639 3.533749 3.524914 3.534874 3.499294\n",
       "  NIR98    NIR99    NIR100  \n",
       "1 2.860067 2.816934 2.814325\n",
       "2 3.214022 3.189640 3.125834\n",
       "3 2.590596 2.558228 2.528773\n",
       "4 2.863643 2.827721 2.795507\n",
       "5 3.181573 3.152868 3.134712\n",
       "6 3.484862 3.467219 3.446953"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = readRDS(\"data/NIR_meat.rds\")\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b55a812d-6535-480d-bac0-2200be6a9713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Train Split\n",
    "set.seed(2023)\n",
    "i <- sample(x=nrow(data), size = NROW(data)/4)\n",
    "test <- dplyr::slice(data, i)\n",
    "train <- dplyr::slice(data, -i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00036e8-d0d3-442a-a2e0-8488f5d3d14d",
   "metadata": {},
   "source": [
    "## Visualizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f40461-2829-4d95-bf38-5c22eda8594e",
   "metadata": {},
   "source": [
    "Visualize and describe the univariate sample distribution of fat content (column fat): what is the mean and the median fat content in the sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c324ffa3-fba4-4794-b772-7bd8d1337c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "17.5283950617284"
      ],
      "text/latex": [
       "17.5283950617284"
      ],
      "text/markdown": [
       "17.5283950617284"
      ],
      "text/plain": [
       "[1] 17.5284"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "12.6"
      ],
      "text/latex": [
       "12.6"
      ],
      "text/markdown": [
       "12.6"
      ],
      "text/plain": [
       "[1] 12.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "155.029002377118"
      ],
      "text/latex": [
       "155.029002377118"
      ],
      "text/markdown": [
       "155.029002377118"
      ],
      "text/plain": [
       "[1] 155.029"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAYAAAD958/bAAAEDmlDQ1BrQ0dDb2xvclNwYWNl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\nECBAgACBFS/wPxbMniJeoM0FAAAAAElFTkSuQmCC",
      "text/plain": [
       "Plot with title “Histogram of train$fat”"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist(train$fat, breaks=30)\n",
    "mean(train$fat)\n",
    "median(train$fat)\n",
    "var(train$fat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f45023-7de7-43d3-8a9c-a72ff336c44e",
   "metadata": {},
   "source": [
    "Each NIR-predictors (aka NIR-features) has its own variance. Across the features, what is the range of these variances? What is the mean variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bde92873-1e9f-428f-b53c-862828530a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.275687332935275"
      ],
      "text/latex": [
       "0.275687332935275"
      ],
      "text/markdown": [
       "0.275687332935275"
      ],
      "text/plain": [
       "[1] 0.2756873"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.154103569426037"
      ],
      "text/latex": [
       "0.154103569426037"
      ],
      "text/markdown": [
       "0.154103569426037"
      ],
      "text/plain": [
       "[1] 0.1541036"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.243098368696132"
      ],
      "text/latex": [
       "0.243098368696132"
      ],
      "text/markdown": [
       "0.243098368696132"
      ],
      "text/plain": [
       "[1] 0.2430984"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vars <- c()\n",
    "for (col in train[-1]) {\n",
    "    vars <- c(vars, var(col))\n",
    "}\n",
    "max(vars)\n",
    "min(vars)\n",
    "mean(vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88089b5f-66dc-4de4-b75a-05e5ffeabf31",
   "metadata": {},
   "source": [
    "Look at the correlations among the NIR-features themselves and also the correlation with the response variable fat. Try to get an overview, for instance, by considering only — say — every 10th feature. What stands out?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18f17a-5e95-4397-92b8-1c231ea0cf15",
   "metadata": {},
   "source": [
    "## Linear Model\n",
    "Fit an additive model with intercept and all available NIR-predictors on the training data and store the model fit. We will use this model as a reference in our benchmark of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "01d5aae9-b08d-45dd-9057-6d4711de5e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.lm <- lm(fat ~ ., data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4551e69d-bd15-44b2-8d77-d0d897c8a9ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = fat ~ ., data = train)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-5.1045 -1.2343  0.2291  1.1686  5.1911 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  20.0424     5.5543   3.608 0.000623 ***\n",
       "NIR1         10.0683    30.3434   0.332 0.741169    \n",
       "NIR2         25.4289    24.6740   1.031 0.306799    \n",
       "NIR3        -35.9336    27.8005  -1.293 0.201041    \n",
       "NIR4         19.0628    31.3012   0.609 0.544779    \n",
       "NIR5        -37.2157    30.0092  -1.240 0.219673    \n",
       "NIR6        -29.4103    26.9023  -1.093 0.278594    \n",
       "NIR7         24.3261    30.6364   0.794 0.430258    \n",
       "NIR8         10.6327    24.3081   0.437 0.663355    \n",
       "NIR9         54.4140    27.6789   1.966 0.053866 .  \n",
       "NIR10        14.1811    35.1660   0.403 0.688165    \n",
       "NIR11       -41.0173    33.4085  -1.228 0.224257    \n",
       "NIR12        20.1472    33.1033   0.609 0.545039    \n",
       "NIR13       -33.3652    28.5899  -1.167 0.247741    \n",
       "NIR14       -52.1544    28.8415  -1.808 0.075488 .  \n",
       "NIR15        34.9914    25.6707   1.363 0.177865    \n",
       "NIR16         5.4291    25.4024   0.214 0.831476    \n",
       "NIR17        -1.0919    33.2725  -0.033 0.973927    \n",
       "NIR18        36.7356    25.4103   1.446 0.153381    \n",
       "NIR19        21.6696    26.9166   0.805 0.423910    \n",
       "NIR20         9.6625    34.2003   0.283 0.778497    \n",
       "NIR21       -41.8531    31.0528  -1.348 0.182705    \n",
       "NIR22         8.9917    29.4803   0.305 0.761400    \n",
       "NIR23        -4.1931    31.4014  -0.134 0.894212    \n",
       "NIR24         3.7281    30.0787   0.124 0.901766    \n",
       "NIR25        -8.9963    26.5792  -0.338 0.736172    \n",
       "NIR26       -43.0050    33.3446  -1.290 0.202018    \n",
       "NIR27        -5.6172    24.7868  -0.227 0.821475    \n",
       "NIR28        -1.9668    29.9299  -0.066 0.947820    \n",
       "NIR29       -58.2333    33.6422  -1.731 0.088515 .  \n",
       "NIR30       -50.4864    28.1175  -1.796 0.077519 .  \n",
       "NIR31        53.4656    26.6468   2.006 0.049246 *  \n",
       "NIR32        31.9570    31.7408   1.007 0.318002    \n",
       "NIR33       -31.2714    29.2833  -1.068 0.289777    \n",
       "NIR34       -10.6672    30.5879  -0.349 0.728487    \n",
       "NIR35        18.1969    36.8433   0.494 0.623150    \n",
       "NIR36       -36.6294    28.6002  -1.281 0.205133    \n",
       "NIR37       -17.4111    28.1527  -0.618 0.538580    \n",
       "NIR38        29.1043    33.7761   0.862 0.392236    \n",
       "NIR39        12.1908    31.0851   0.392 0.696296    \n",
       "NIR40        57.0605    26.4083   2.161 0.034655 *  \n",
       "NIR41         0.9074    27.1580   0.033 0.973456    \n",
       "NIR42        27.2943    33.0698   0.825 0.412387    \n",
       "NIR43        59.8669    31.7873   1.883 0.064421 .  \n",
       "NIR44        37.5739    36.0663   1.042 0.301615    \n",
       "NIR45        57.3272    34.6134   1.656 0.102814    \n",
       "NIR46        -4.8684    25.4657  -0.191 0.849022    \n",
       "NIR47       -39.8494    26.1462  -1.524 0.132652    \n",
       "NIR48       -86.5741    35.4300  -2.444 0.017453 *  \n",
       "NIR49        -3.4592    30.1077  -0.115 0.908906    \n",
       "NIR50       -39.3649    28.1520  -1.398 0.167087    \n",
       "NIR51       -42.4598    28.2054  -1.505 0.137388    \n",
       "NIR52         8.4162    29.4841   0.285 0.776267    \n",
       "NIR53       -16.6259    32.9123  -0.505 0.615268    \n",
       "NIR54        58.3767    35.5007   1.644 0.105242    \n",
       "NIR55        61.7119    30.1272   2.048 0.044833 *  \n",
       "NIR56        10.3952    31.6297   0.329 0.743544    \n",
       "NIR57       -42.3433    31.2714  -1.354 0.180712    \n",
       "NIR58        -1.3686    25.7591  -0.053 0.957800    \n",
       "NIR59        39.8298    33.7548   1.180 0.242592    \n",
       "NIR60        14.4677    27.0801   0.534 0.595108    \n",
       "NIR61        -5.1300    25.9754  -0.197 0.844098    \n",
       "NIR62        26.3779    34.7162   0.760 0.450291    \n",
       "NIR63       -53.9092    27.0978  -1.989 0.051143 .  \n",
       "NIR64         3.8883    28.9193   0.134 0.893486    \n",
       "NIR65       -22.4985    31.2723  -0.719 0.474619    \n",
       "NIR66        32.3402    30.8509   1.048 0.298648    \n",
       "NIR67        36.2543    26.0948   1.389 0.169784    \n",
       "NIR68        22.3668    28.3087   0.790 0.432529    \n",
       "NIR69        36.2725    28.3383   1.280 0.205397    \n",
       "NIR70       -47.5650    30.0251  -1.584 0.118325    \n",
       "NIR71        -7.3668    28.0784  -0.262 0.793924    \n",
       "NIR72       -43.0361    27.2541  -1.579 0.119493    \n",
       "NIR73         1.4673    30.3403   0.048 0.961586    \n",
       "NIR74       -25.5781    29.1571  -0.877 0.383792    \n",
       "NIR75       -32.5182    32.7021  -0.994 0.323968    \n",
       "NIR76       -20.4074    25.8015  -0.791 0.432045    \n",
       "NIR77        -9.2120    30.3222  -0.304 0.762311    \n",
       "NIR78        25.0277    30.4863   0.821 0.414872    \n",
       "NIR79       -43.9578    28.9706  -1.517 0.134350    \n",
       "NIR80       -19.3227    26.9951  -0.716 0.476853    \n",
       "NIR81         3.4576    30.0958   0.115 0.908913    \n",
       "NIR82        -1.3596    27.7214  -0.049 0.961044    \n",
       "NIR83       -19.4368    32.6830  -0.595 0.554239    \n",
       "NIR84        32.6759    31.1259   1.050 0.297953    \n",
       "NIR85        16.7526    28.3125   0.592 0.556236    \n",
       "NIR86        -5.3266    26.8434  -0.198 0.843367    \n",
       "NIR87        -5.0538    28.4902  -0.177 0.859792    \n",
       "NIR88         7.6712    28.6629   0.268 0.789883    \n",
       "NIR89       -32.3550    21.9450  -1.474 0.145526    \n",
       "NIR90        20.2197    38.3840   0.527 0.600262    \n",
       "NIR91        -3.8090    32.1286  -0.119 0.906019    \n",
       "NIR92        54.7720    29.3404   1.867 0.066741 .  \n",
       "NIR93        17.4250    24.5700   0.709 0.480903    \n",
       "NIR94       -25.7621    27.0350  -0.953 0.344393    \n",
       "NIR95        15.7277    26.8393   0.586 0.560040    \n",
       "NIR96        11.2565    31.0504   0.363 0.718214    \n",
       "NIR97        54.2636    33.1713   1.636 0.107019    \n",
       "NIR98       -21.1838    28.1920  -0.751 0.455294    \n",
       "NIR99        -3.4847    27.4252  -0.127 0.899309    \n",
       "NIR100      -31.3140    28.1581  -1.112 0.270468    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 3.068 on 61 degrees of freedom\n",
       "Multiple R-squared:  0.977,\tAdjusted R-squared:  0.9393 \n",
       "F-statistic: 25.91 on 100 and 61 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(m.lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7918a3-37f7-46e2-8b4d-175e4662bd5c",
   "metadata": {},
   "source": [
    "## Variable Selection\n",
    "All absorbance predictors are potentially relevant predictors for fat-level and hence the full linear model seems like the best linear additive model. Nevertheless, we try out two variable selection methods to build two simpler regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92766c-f8bf-407d-9b40-2540b25ef114",
   "metadata": {},
   "source": [
    "### Using AIC\n",
    "The R-function step uses — starting from the provided model fit — the AIC-criterion to decide if dropping or adding a variable improves the model. The AIC-criterion balances model fit on the training data with model complexity. Lower values of AIC indicate better fits. For a fitted model with p parameters, AIC is defined as \n",
    "\n",
    "$AIC = −2 × max(loglikelihood) + 2 × p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "26ab0a01-b640-4d21-bb86-cae4b5a89df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.aic <- step(m.lm, direction = \"both\", k = 2, trace = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9d92b894-e9b4-4d4a-9f16-44a47e93c8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = fat ~ NIR2 + NIR3 + NIR4 + NIR5 + NIR7 + NIR9 + \n",
       "    NIR11 + NIR13 + NIR14 + NIR15 + NIR18 + NIR19 + NIR20 + NIR21 + \n",
       "    NIR26 + NIR29 + NIR30 + NIR31 + NIR32 + NIR33 + NIR36 + NIR38 + \n",
       "    NIR40 + NIR42 + NIR43 + NIR44 + NIR45 + NIR47 + NIR48 + NIR50 + \n",
       "    NIR51 + NIR53 + NIR54 + NIR55 + NIR57 + NIR59 + NIR63 + NIR66 + \n",
       "    NIR67 + NIR69 + NIR70 + NIR72 + NIR74 + NIR75 + NIR78 + NIR79 + \n",
       "    NIR80 + NIR84 + NIR89 + NIR92 + NIR94 + NIR97 + NIR98, data = train)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)         NIR2         NIR3         NIR4         NIR5         NIR7  \n",
       "      21.47        29.07       -25.27        35.22       -42.61        29.40  \n",
       "       NIR9        NIR11        NIR13        NIR14        NIR15        NIR18  \n",
       "      45.74       -27.20       -50.51       -52.74        34.12        34.73  \n",
       "      NIR19        NIR20        NIR21        NIR26        NIR29        NIR30  \n",
       "      20.33        20.99       -47.67       -43.95       -64.00       -44.66  \n",
       "      NIR31        NIR32        NIR33        NIR36        NIR38        NIR40  \n",
       "      35.86        39.44       -27.81       -39.08        31.21        58.36  \n",
       "      NIR42        NIR43        NIR44        NIR45        NIR47        NIR48  \n",
       "      35.36        51.08        31.75        61.03       -43.63       -56.15  \n",
       "      NIR50        NIR51        NIR53        NIR54        NIR55        NIR57  \n",
       "     -40.07       -31.43       -31.79        59.74        66.61       -42.73  \n",
       "      NIR59        NIR63        NIR66        NIR67        NIR69        NIR70  \n",
       "      43.58       -44.13        34.60        37.84        30.64       -42.09  \n",
       "      NIR72        NIR74        NIR75        NIR78        NIR79        NIR80  \n",
       "     -37.72       -36.11       -34.62        25.12       -25.58       -25.48  \n",
       "      NIR84        NIR89        NIR92        NIR94        NIR97        NIR98  \n",
       "      22.12       -31.02        66.86       -23.65        52.51       -22.83  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 53 predictors left\n",
    "m.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b453f7-310d-43b7-8522-42e9f003aa1f",
   "metadata": {},
   "source": [
    "### Using BIC\n",
    "Use the parameter $k$ of the step-function to adjust the penalization factor to get BIC instead of AIC as variable selection criterion. Confirm that the resulting model is sparser than the model selected by AIC.\n",
    "How many predictors in the BIC-model are statistically significant at the significance level of 5%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ea510bb2-694b-473a-86ff-f246ea0297ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.bic <- step(m.lm, direction = \"both\", k = log(length(train)), trace = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0512539c-b3d7-469f-9fd0-ed3be8d03fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = fat ~ NIR4 + NIR13 + NIR21 + NIR26 + NIR29 + NIR30 + \n",
       "    NIR40 + NIR42 + NIR45 + NIR50 + NIR74 + NIR92 + NIR38, data = train)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)         NIR4        NIR13        NIR21        NIR26        NIR29  \n",
       "      23.64        66.86       -47.55       -43.28       -41.45       -54.20  \n",
       "      NIR30        NIR40        NIR42        NIR45        NIR50        NIR74  \n",
       "     -36.55        38.89        72.29        66.70       -48.80       -48.72  \n",
       "      NIR92        NIR38  \n",
       "      31.94        41.92  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.bic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b411bbf-7c51-44c3-a406-4ac8a19feae6",
   "metadata": {},
   "source": [
    "## Shrinkage Methods\n",
    "Variable selection either keeps a variable or drops it completely from the model, — it is either-or. The R-package `glmnet` allows for different ways to continuously shrink parameter coefficient estimates of a linear regression model. The main functions of the package are `glmnet` and `cv.glmnet`.\n",
    "\n",
    "Within these functions the parameter alpha= specifies the type of continuous penalization:\n",
    "\n",
    "- `alpha=0` corresponds to ridge regression.\n",
    "- `alpha=1` is Lasso-regression.\n",
    "- a value between 0 and 1 corresponds to a mixture of ridge and Lasso-regression and this approach to penalization is called elastic net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "209bca37-fed9-4500-af26-d3069c0295d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"glmnet\")\n",
    "library(\"dplyr\")\n",
    "\n",
    "y_glm_train <- train |> dplyr::pull(fat)\n",
    "x_glm_train <- train |> dplyr::select(!fat) |> as.matrix()\n",
    "y_glm_test <- test |> dplyr::pull(fat)\n",
    "x_glm_test <- test |> dplyr::select(!fat) |> as.matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d568bbe-8a2b-45e4-9804-f49b9ac0130a",
   "metadata": {},
   "source": [
    "Concretely, fit three different models in the following order:\n",
    "- Ridge regression\n",
    "- Lasso-regression\n",
    "- Elastic net as mixture of ridge and Lasso-regression, with mixing parameter $α$ = 0.3. (With $α$ = 0.3, we choose a stronger contribution of ridge regression because we have so strongly correlated predictors where ridge regression is designed for.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "764e95ac-baed-4ede-940c-78d1ac42c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.glm_ridge <- cv.glmnet(x_glm_train, y_glm_train, alpha = 0)\n",
    "m.glm_lasso <- cv.glmnet(x_glm_train, y_glm_train, alpha = 1)\n",
    "m.glm_elastic <- cv.glmnet(x_glm_train, y_glm_train, alpha = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc109596-42d6-4ad1-be58-9091f0df16e4",
   "metadata": {},
   "source": [
    "Identify for each of the three models classes the value for the penalization parameter $λ$ so that the cross-validated prediction error is within one standard error of the minimum prediction error.\n",
    "\n",
    "How are the model coefficients affected by the penalization? Compare the estimated coefficients with those from the full linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0835ed-d40b-4f55-bbbd-1e99292749a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d701dfd9-5a03-479e-8de4-5af033a1aa32",
   "metadata": {},
   "source": [
    "## Model Assessment\n",
    "We have fitted different models on the NIR-training data. Now, we want to use the reserved test data to assess these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ddad3a7e-2189-4456-83fe-4afc1bfdc143",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual <- function(predictions, actual) sum((predictions - actual)^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "445b14b3-d62e-43c8-94b5-aff0b6119a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "615.085480056521"
      ],
      "text/latex": [
       "615.085480056521"
      ],
      "text/markdown": [
       "615.085480056521"
      ],
      "text/plain": [
       "[1] 615.0855"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "residual(predict(m.glm_elastic, newx = x_glm_test), y_glm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5550a94e-00ad-4eea-97c2-a5c9f86277de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "621.548966674841"
      ],
      "text/latex": [
       "621.548966674841"
      ],
      "text/markdown": [
       "621.548966674841"
      ],
      "text/plain": [
       "[1] 621.549"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "residual(predict(m.glm_lasso, newx = x_glm_test), y_glm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "354ae98c-4f23-4aa8-8c5e-0de30c1d3fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "777.131112044752"
      ],
      "text/latex": [
       "777.131112044752"
      ],
      "text/markdown": [
       "777.131112044752"
      ],
      "text/plain": [
       "[1] 777.1311"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "residual(predict.lm(m.bic, newdata = test), test$fat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "361698b8-dd10-432d-8016-94ec37239e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1324.12063614704"
      ],
      "text/latex": [
       "1324.12063614704"
      ],
      "text/markdown": [
       "1324.12063614704"
      ],
      "text/plain": [
       "[1] 1324.121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "residual(predict.lm(m.aic, newdata = test), test$fat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1747a3b8-21c8-4073-8cc0-37248eab1e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1352.55002725996"
      ],
      "text/latex": [
       "1352.55002725996"
      ],
      "text/markdown": [
       "1352.55002725996"
      ],
      "text/plain": [
       "[1] 1352.55"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "residual(predict(m.glm_ridge, newx = x_glm_test), y_glm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a9e8a756-1b82-4902-bc71-e85d858d48dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1647.87610579269"
      ],
      "text/latex": [
       "1647.87610579269"
      ],
      "text/markdown": [
       "1647.87610579269"
      ],
      "text/plain": [
       "[1] 1647.876"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "residual(predict.lm(m.lm, newdata = test), test$fat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dafa04-b9cd-45e3-9f70-067b297c10c7",
   "metadata": {},
   "source": [
    "### Ranking\n",
    "1. Elastic with $\\alpha = 0.3$\n",
    "2. Lasso\n",
    "3. BIC\n",
    "5. AIC\n",
    "4. Ridge\n",
    "6. Simple LM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbda0f8-30a4-4aed-b30d-29d3106963d4",
   "metadata": {},
   "source": [
    "# Practice Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ebf69ac2-fa8b-404f-9c99-67b858387bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "-86.5741112790928"
      ],
      "text/latex": [
       "-86.5741112790928"
      ],
      "text/markdown": [
       "-86.5741112790928"
      ],
      "text/plain": [
       "[1] -86.57411"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "61.711872899151"
      ],
      "text/latex": [
       "61.711872899151"
      ],
      "text/markdown": [
       "61.711872899151"
      ],
      "text/plain": [
       "[1] 61.71187"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "min(m.lm$coefficients)\n",
    "max(m.lm$coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "dd8e7501-c04e-4559-8dcb-160f5917d741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "54"
      ],
      "text/latex": [
       "54"
      ],
      "text/markdown": [
       "54"
      ],
      "text/plain": [
       "[1] 54"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length(m.aic$coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1d6c50cf-0612-42f6-94ee-aca2c29b7780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "14"
      ],
      "text/latex": [
       "14"
      ],
      "text/markdown": [
       "14"
      ],
      "text/plain": [
       "[1] 14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length(m.bic$coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "34b1901c-74c9-408b-9fd8-df0e1f395a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:  cv.glmnet(x = x_glm_train, y = y_glm_train, alpha = 0) \n",
       "\n",
       "Measure: Mean-Squared Error \n",
       "\n",
       "    Lambda Index Measure    SE Nonzero\n",
       "min 0.8898    96   27.66 4.031     100\n",
       "1se 1.1762    93   31.34 5.051     100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.glm_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "43a8a766-053c-4d67-b9ca-d289575bc126",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.glm_ridge_lambda <- cv.glmnet(x_glm_train, y_glm_train, lambda = c(0.8898, 1.1762), alpha = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "af3b367d-1c63-4f1a-9b07-b8495c3c80cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:  cv.glmnet(x = x_glm_train, y = y_glm_train, lambda = c(0.8898,      1.1762), alpha = 0) \n",
      "\n",
      "Measure: Mean-Squared Error \n",
      "\n",
      "    Lambda Index Measure  SE Nonzero\n",
      "min 0.8898     2   27.95 4.2     100\n",
      "1se 0.8898     2   27.95 4.2     100\n"
     ]
    }
   ],
   "source": [
    "print(m.glm_ridge_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3b2d8304-a58d-448f-a3d3-559e2bfdcada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table style=\"width: 100%;\"><tr><td>cv.glmnet {glmnet}</td><td style=\"text-align: right;\">R Documentation</td></tr></table>\n",
       "\n",
       "<h2 id='cv.glmnet'>Cross-validation for glmnet</h2>\n",
       "\n",
       "<h3>Description</h3>\n",
       "\n",
       "<p>Does k-fold cross-validation for glmnet, produces a plot, and returns a\n",
       "value for <code>lambda</code> (and <code>gamma</code> if <code>relax=TRUE</code>)\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Usage</h3>\n",
       "\n",
       "<pre><code class='language-R'>cv.glmnet(\n",
       "  x,\n",
       "  y,\n",
       "  weights = NULL,\n",
       "  offset = NULL,\n",
       "  lambda = NULL,\n",
       "  type.measure = c(\"default\", \"mse\", \"deviance\", \"class\", \"auc\", \"mae\", \"C\"),\n",
       "  nfolds = 10,\n",
       "  foldid = NULL,\n",
       "  alignment = c(\"lambda\", \"fraction\"),\n",
       "  grouped = TRUE,\n",
       "  keep = FALSE,\n",
       "  parallel = FALSE,\n",
       "  gamma = c(0, 0.25, 0.5, 0.75, 1),\n",
       "  relax = FALSE,\n",
       "  trace.it = 0,\n",
       "  ...\n",
       ")\n",
       "</code></pre>\n",
       "\n",
       "\n",
       "<h3>Arguments</h3>\n",
       "\n",
       "<table>\n",
       "<tr><td><code id=\"cv.glmnet_:_x\">x</code></td>\n",
       "<td>\n",
       "<p><code>x</code> matrix as in <code>glmnet</code>.</p>\n",
       "</td></tr>\n",
       "<tr><td><code id=\"cv.glmnet_:_y\">y</code></td>\n",
       "<td>\n",
       "<p>response <code>y</code> as in <code>glmnet</code>.</p>\n",
       "</td></tr>\n",
       "<tr><td><code id=\"cv.glmnet_:_weights\">weights</code></td>\n",
       "<td>\n",
       "<p>Observation weights; defaults to 1 per observation</p>\n",
       "</td></tr>\n",
       "<tr><td><code id=\"cv.glmnet_:_offset\">offset</code></td>\n",
       "<td>\n",
       "<p>Offset vector (matrix) as in <code>glmnet</code></p>\n",
       "</td></tr>\n",
       "<tr><td><code id=\"cv.glmnet_:_lambda\">lambda</code></td>\n",
       "<td>\n",
       "<p>Optional user-supplied lambda sequence; default is\n",
       "<code>NULL</code>, and <code>glmnet</code> chooses its own sequence. Note that this is done\n",
       "for the full model (master sequence), and separately for each fold.\n",
       "The fits are then alligned using the master sequence (see the <code>allignment</code>\n",
       "argument for additional details). Adapting <code>lambda</code> for each fold\n",
       "leads to better convergence. When <code>lambda</code> is supplied, the same sequence\n",
       "is used everywhere, but in some GLMs can lead to convergence issues.</p>\n",
       "</td></tr>\n",
       "<tr><td><code id=\"cv.glmnet_:_type.measure\">type.measure</code></td>\n",
       "<td>\n",
       "<p>loss to use for cross-validation. Currently five\n",
       "options, not all available for all models. The default is\n",
       "<code>type.measure=\"deviance\"</code>, which uses squared-error for gaussian models\n",
       "(a.k.a <code>type.measure=\"mse\"</code> there), deviance for logistic and poisson\n",
       "regression, and partial-likelihood for the Cox model.\n",
       "<code>type.measure=\"class\"</code> applies to binomial and multinomial logistic\n",
       "regression only, and gives misclassification error.\n",
       "<code>type.measure=\"auc\"</code> is for two-class logistic regression only, and\n",
       "gives area under the ROC curve. <code>type.measure=\"mse\"</code> or\n",
       "<code>type.measure=\"mae\"</code> (mean absolute error) can be used by all models\n",
       "except the <code>\"cox\"</code>; they measure the deviation from the fitted mean to\n",
       "the response.\n",
       "<code>type.measure=\"C\"</code> is Harrel's concordance measure, only available for <code>cox</code> models.</p>\n",
       "</td></tr>\n",
       "<tr><td><code id=\"cv.glmnet_:_nfolds\">nfolds</code></td>\n",
       "<td>\n",
       "<p>number of folds - default is 10. Although <code>nfolds</code> can be\n",
       "as large as the sample size (leave-one-out CV), it is not recommended for\n",
       "large datasets. Smallest value allowable is <code>nfolds=3</code></p>\n",
       "</td></tr>\n",
       "<tr><td><code id=\"cv.glmnet_:_foldid\">foldid</code></td>\n",
       "<td>\n",
       "<p>an optional vector of values between 1 and <code>nfolds</code>\n",
       "identifying what fold each observation is in. If supplied, <code>nfolds</code> can\n",
       "be missing.</p>\n",
       "</td></tr>\n",
       "<tr><td><code id=\"cv.glmnet_:_alignment\">alignment</code></td>\n",
       "<td>\n",
       "<p>This is an experimental argument, designed to fix the\n",
       "problems users were having with CV, with possible values <code>\"lambda\"</code>\n",
       "(the default) else <code>\"fraction\"</code>. With <code>\"lambda\"</code> the <code>lambda</code>\n",
       "values from the master fit (on all the data) are used to line up the\n",
       "predictions from each of the folds. In some cases this can give strange\n",
       "values, since the effective <code>lambda</code> values in each fold could be quite\n",
       "different. With <code>\"fraction\"</code> we line up the predictions in each fold\n",
       "according to the fraction of progress along the regularization. If in the\n",
       "call a <code>lambda</code> argument is also provided, <code>alignment=\"fraction\"</code>\n",
       "is ignored (with a warning).</p>\n",
       "</td></tr>\n",
       "<tr><td><code id=\"cv.glmnet_:_grouped\">grouped</code></td>\n",
       "<td>\n",
       "<p>This is an experimental argument, with default <code>TRUE</code>,\n",
       "and can be ignored by most users. For all models except the <code>\"cox\"</code>,\n",
       "this refers to computing <code>nfolds</code> separate statistics, and then using\n",
       "their mean and estimated standard error to describe the CV curve. If\n",
       "<code>grouped=FALSE</code>, an error matrix is built up at the observation level\n",
       "from the predictions from the <code>nfolds</code> fits, and then summarized (does\n",
       "not apply to <code>type.measure=\"auc\"</code>). For the <code>\"cox\"</code> family,\n",
       "<code>grouped=TRUE</code> obtains the CV partial likelihood for the Kth fold by\n",
       "<em>subtraction</em>; by subtracting the log partial likelihood evaluated on\n",
       "the full dataset from that evaluated on the on the (K-1)/K dataset. This\n",
       "makes more efficient use of risk sets. With <code>grouped=FALSE</code> the log\n",
       "partial likelihood is computed only on the Kth fold</p>\n",
       "</td></tr>\n",
       "<tr><td><code id=\"cv.glmnet_:_keep\">keep</code></td>\n",
       "<td>\n",
       "<p>If <code>keep=TRUE</code>, a <em>prevalidated</em> array is returned\n",
       "containing fitted values for each observation and each value of\n",
       "<code>lambda</code>. This means these fits are computed with this observation and\n",
       "the rest of its fold omitted. The <code>foldid</code> vector is also returned.\n",
       "Default is keep=FALSE.  If <code>relax=TRUE</code>, then a list of such arrays is\n",
       "returned, one for each value of 'gamma'. Note: if the value 'gamma=1' is\n",
       "omitted, this case is included in the list since it corresponds to the\n",
       "original 'glmnet' fit.</p>\n",
       "</td></tr>\n",
       "<tr><td><code id=\"cv.glmnet_:_parallel\">parallel</code></td>\n",
       "<td>\n",
       "<p>If <code>TRUE</code>, use parallel <code>foreach</code> to fit each\n",
       "fold.  Must register parallel before hand, such as <code>doMC</code> or others.\n",
       "See the example below.</p>\n",
       "</td></tr>\n",
       "<tr><td><code id=\"cv.glmnet_:_gamma\">gamma</code></td>\n",
       "<td>\n",
       "<p>The values of the parameter for mixing the relaxed fit with the\n",
       "regularized fit, between 0 and 1; default is <code>gamma = c(0, 0.25, 0.5,\n",
       "0.75, 1)</code></p>\n",
       "</td></tr>\n",
       "<tr><td><code id=\"cv.glmnet_:_relax\">relax</code></td>\n",
       "<td>\n",
       "<p>If <code>TRUE</code>, then CV is done with respect to the mixing\n",
       "parameter <code>gamma</code> as well as <code>lambda</code>. Default is\n",
       "<code>relax=FALSE</code></p>\n",
       "</td></tr>\n",
       "<tr><td><code id=\"cv.glmnet_:_trace.it\">trace.it</code></td>\n",
       "<td>\n",
       "<p>If <code>trace.it=1</code>, then progress bars are displayed;\n",
       "useful for big models that take a long time to fit. Limited tracing if\n",
       "<code>parallel=TRUE</code></p>\n",
       "</td></tr>\n",
       "<tr><td><code id=\"cv.glmnet_:_...\">...</code></td>\n",
       "<td>\n",
       "<p>Other arguments that can be passed to <code>glmnet</code></p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Details</h3>\n",
       "\n",
       "<p>The function runs <code>glmnet</code> <code>nfolds</code>+1 times; the first to get the\n",
       "<code>lambda</code> sequence, and then the remainder to compute the fit with each\n",
       "of the folds omitted. The error is accumulated, and the average error and\n",
       "standard deviation over the folds is computed.  Note that <code>cv.glmnet</code>\n",
       "does NOT search for values for <code>alpha</code>. A specific value should be\n",
       "supplied, else <code>alpha=1</code> is assumed by default. If users would like to\n",
       "cross-validate <code>alpha</code> as well, they should call <code>cv.glmnet</code> with\n",
       "a pre-computed vector <code>foldid</code>, and then use this same fold vector in\n",
       "separate calls to <code>cv.glmnet</code> with different values of <code>alpha</code>.\n",
       "Note also that the results of <code>cv.glmnet</code> are random, since the folds\n",
       "are selected at random. Users can reduce this randomness by running\n",
       "<code>cv.glmnet</code> many times, and averaging the error curves.\n",
       "</p>\n",
       "<p>If <code>relax=TRUE</code> then the values of <code>gamma</code> are used to mix the\n",
       "fits. If <code class=\"reqn\">\\eta</code> is the fit for lasso/elastic net, and <code class=\"reqn\">\\eta_R</code> is\n",
       "the relaxed fit (with unpenalized coefficients), then a relaxed fit mixed by\n",
       "<code class=\"reqn\">\\gamma</code> is </p>\n",
       "<p style=\"text-align: center;\"><code class=\"reqn\">\\eta(\\gamma)=(1-\\gamma)\\eta_R+\\gamma\\eta.</code>\n",
       "</p>\n",
       "<p> There is\n",
       "practically no extra cost for having a lot of values for <code>gamma</code>.\n",
       "However, 5 seems sufficient for most purposes. CV then selects both\n",
       "<code>gamma</code> and <code>lambda</code>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Value</h3>\n",
       "\n",
       "<p>an object of class <code>\"cv.glmnet\"</code> is returned, which is a list\n",
       "with the ingredients of the cross-validation fit.  If the object was created\n",
       "with <code>relax=TRUE</code> then this class has a prefix class of\n",
       "<code>\"cv.relaxed\"</code>.  </p>\n",
       "<table>\n",
       "<tr><td><code>lambda</code></td>\n",
       "<td>\n",
       "<p>the values of <code>lambda</code> used in the\n",
       "fits.</p>\n",
       "</td></tr> <tr><td><code>cvm</code></td>\n",
       "<td>\n",
       "<p>The mean cross-validated error - a vector of length\n",
       "<code>length(lambda)</code>.</p>\n",
       "</td></tr> <tr><td><code>cvsd</code></td>\n",
       "<td>\n",
       "<p>estimate of standard error of\n",
       "<code>cvm</code>.</p>\n",
       "</td></tr> <tr><td><code>cvup</code></td>\n",
       "<td>\n",
       "<p>upper curve = <code>cvm+cvsd</code>.</p>\n",
       "</td></tr> <tr><td><code>cvlo</code></td>\n",
       "<td>\n",
       "<p>lower\n",
       "curve = <code>cvm-cvsd</code>.</p>\n",
       "</td></tr> <tr><td><code>nzero</code></td>\n",
       "<td>\n",
       "<p>number of non-zero coefficients at\n",
       "each <code>lambda</code>.</p>\n",
       "</td></tr> <tr><td><code>name</code></td>\n",
       "<td>\n",
       "<p>a text string indicating type of measure\n",
       "(for plotting purposes).</p>\n",
       "</td></tr> <tr><td><code>glmnet.fit</code></td>\n",
       "<td>\n",
       "<p>a fitted glmnet object for the\n",
       "full data.</p>\n",
       "</td></tr> <tr><td><code>lambda.min</code></td>\n",
       "<td>\n",
       "<p>value of <code>lambda</code> that gives minimum\n",
       "<code>cvm</code>.</p>\n",
       "</td></tr> <tr><td><code>lambda.1se</code></td>\n",
       "<td>\n",
       "<p>largest value of <code>lambda</code> such that\n",
       "error is within 1 standard error of the minimum.</p>\n",
       "</td></tr> <tr><td><code>fit.preval</code></td>\n",
       "<td>\n",
       "<p>if\n",
       "<code>keep=TRUE</code>, this is the array of prevalidated fits. Some entries can\n",
       "be <code>NA</code>, if that and subsequent values of <code>lambda</code> are not reached\n",
       "for that fold</p>\n",
       "</td></tr> <tr><td><code>foldid</code></td>\n",
       "<td>\n",
       "<p>if <code>keep=TRUE</code>, the fold assignments used</p>\n",
       "</td></tr>\n",
       "<tr><td><code>index</code></td>\n",
       "<td>\n",
       "<p>a one column matrix with the indices of <code>lambda.min</code> and <code>lambda.1se</code> in the sequence of coefficients, fits etc.</p>\n",
       "</td></tr>\n",
       "<tr><td><code>relaxed</code></td>\n",
       "<td>\n",
       "<p>if <code>relax=TRUE</code>, this additional item has the CV info\n",
       "for each of the mixed fits. In particular it also selects <code>lambda,\n",
       "gamma</code> pairs corresponding to the 1se rule, as well as the minimum error. It also has a component <code>index</code>, a two-column matrix  which contains the <code>lambda</code> and <code>gamma</code> indices corresponding to the &quot;min&quot; and &quot;1se&quot; solutions.</p>\n",
       "</td></tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "<h3>Author(s)</h3>\n",
       "\n",
       "<p>Jerome Friedman, Trevor Hastie and Rob Tibshirani<br /> Noah Simon\n",
       "helped develop the 'coxnet' function.<br /> Jeffrey Wong and B. Narasimhan\n",
       "helped with the parallel option<br /> Maintainer: Trevor Hastie\n",
       "<a href=\"mailto:hastie@stanford.edu\">hastie@stanford.edu</a>\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>References</h3>\n",
       "\n",
       "<p>Friedman, J., Hastie, T. and Tibshirani, R. (2008)\n",
       "<em>Regularization Paths for Generalized Linear Models via Coordinate\n",
       "Descent (2010), Journal of Statistical Software, Vol. 33(1), 1-22</em>,\n",
       "<a href=\"https://doi.org/10.18637/jss.v033.i01\">doi:10.18637/jss.v033.i01</a>.<br />\n",
       "Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2011)\n",
       "<em>Regularization Paths for Cox's Proportional\n",
       "Hazards Model via Coordinate Descent, Journal of Statistical Software, Vol.\n",
       "39(5), 1-13</em>,\n",
       "<a href=\"https://doi.org/10.18637/jss.v039.i05\">doi:10.18637/jss.v039.i05</a>.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>See Also</h3>\n",
       "\n",
       "<p><code>glmnet</code> and <code>plot</code>, <code>predict</code>, and <code>coef</code>\n",
       "methods for <code>\"cv.glmnet\"</code> and <code>\"cv.relaxed\"</code> objects.\n",
       "</p>\n",
       "\n",
       "\n",
       "<h3>Examples</h3>\n",
       "\n",
       "<pre><code class='language-R'>\n",
       "set.seed(1010)\n",
       "n = 1000\n",
       "p = 100\n",
       "nzc = trunc(p/10)\n",
       "x = matrix(rnorm(n * p), n, p)\n",
       "beta = rnorm(nzc)\n",
       "fx = x[, seq(nzc)] %*% beta\n",
       "eps = rnorm(n) * 5\n",
       "y = drop(fx + eps)\n",
       "px = exp(fx)\n",
       "px = px/(1 + px)\n",
       "ly = rbinom(n = length(px), prob = px, size = 1)\n",
       "set.seed(1011)\n",
       "cvob1 = cv.glmnet(x, y)\n",
       "plot(cvob1)\n",
       "coef(cvob1)\n",
       "predict(cvob1, newx = x[1:5, ], s = \"lambda.min\")\n",
       "title(\"Gaussian Family\", line = 2.5)\n",
       "set.seed(1011)\n",
       "cvob1a = cv.glmnet(x, y, type.measure = \"mae\")\n",
       "plot(cvob1a)\n",
       "title(\"Gaussian Family\", line = 2.5)\n",
       "set.seed(1011)\n",
       "par(mfrow = c(2, 2), mar = c(4.5, 4.5, 4, 1))\n",
       "cvob2 = cv.glmnet(x, ly, family = \"binomial\")\n",
       "plot(cvob2)\n",
       "title(\"Binomial Family\", line = 2.5)\n",
       "frame()\n",
       "set.seed(1011)\n",
       "cvob3 = cv.glmnet(x, ly, family = \"binomial\", type.measure = \"class\")\n",
       "plot(cvob3)\n",
       "title(\"Binomial Family\", line = 2.5)\n",
       "## Not run: \n",
       "cvob1r = cv.glmnet(x, y, relax = TRUE)\n",
       "plot(cvob1r)\n",
       "predict(cvob1r, newx = x[, 1:5])\n",
       "set.seed(1011)\n",
       "cvob3a = cv.glmnet(x, ly, family = \"binomial\", type.measure = \"auc\")\n",
       "plot(cvob3a)\n",
       "title(\"Binomial Family\", line = 2.5)\n",
       "set.seed(1011)\n",
       "mu = exp(fx/10)\n",
       "y = rpois(n, mu)\n",
       "cvob4 = cv.glmnet(x, y, family = \"poisson\")\n",
       "plot(cvob4)\n",
       "title(\"Poisson Family\", line = 2.5)\n",
       "\n",
       "# Multinomial\n",
       "n = 500\n",
       "p = 30\n",
       "nzc = trunc(p/10)\n",
       "x = matrix(rnorm(n * p), n, p)\n",
       "beta3 = matrix(rnorm(30), 10, 3)\n",
       "beta3 = rbind(beta3, matrix(0, p - 10, 3))\n",
       "f3 = x %*% beta3\n",
       "p3 = exp(f3)\n",
       "p3 = p3/apply(p3, 1, sum)\n",
       "g3 = glmnet:::rmult(p3)\n",
       "set.seed(10101)\n",
       "cvfit = cv.glmnet(x, g3, family = \"multinomial\")\n",
       "plot(cvfit)\n",
       "title(\"Multinomial Family\", line = 2.5)\n",
       "# Cox\n",
       "beta = rnorm(nzc)\n",
       "fx = x[, seq(nzc)] %*% beta/3\n",
       "hx = exp(fx)\n",
       "ty = rexp(n, hx)\n",
       "tcens = rbinom(n = n, prob = 0.3, size = 1)  # censoring indicator\n",
       "y = cbind(time = ty, status = 1 - tcens)  # y=Surv(ty,1-tcens) with library(survival)\n",
       "foldid = sample(rep(seq(10), length = n))\n",
       "fit1_cv = cv.glmnet(x, y, family = \"cox\", foldid = foldid)\n",
       "plot(fit1_cv)\n",
       "title(\"Cox Family\", line = 2.5)\n",
       "# Parallel\n",
       "require(doMC)\n",
       "registerDoMC(cores = 4)\n",
       "x = matrix(rnorm(1e+05 * 100), 1e+05, 100)\n",
       "y = rnorm(1e+05)\n",
       "system.time(cv.glmnet(x, y))\n",
       "system.time(cv.glmnet(x, y, parallel = TRUE))\n",
       "\n",
       "## End(Not run)\n",
       "\n",
       "</code></pre>\n",
       "\n",
       "<hr /><div style=\"text-align: center;\">[Package <em>glmnet</em> version 4.1-8 ]</div>\n",
       "</div>"
      ],
      "text/latex": [
       "\\inputencoding{utf8}\n",
       "\\HeaderA{cv.glmnet}{Cross-validation for glmnet}{cv.glmnet}\n",
       "\\keyword{models}{cv.glmnet}\n",
       "\\keyword{regression}{cv.glmnet}\n",
       "%\n",
       "\\begin{Description}\n",
       "Does k-fold cross-validation for glmnet, produces a plot, and returns a\n",
       "value for \\code{lambda} (and \\code{gamma} if \\code{relax=TRUE})\n",
       "\\end{Description}\n",
       "%\n",
       "\\begin{Usage}\n",
       "\\begin{verbatim}\n",
       "cv.glmnet(\n",
       "  x,\n",
       "  y,\n",
       "  weights = NULL,\n",
       "  offset = NULL,\n",
       "  lambda = NULL,\n",
       "  type.measure = c(\"default\", \"mse\", \"deviance\", \"class\", \"auc\", \"mae\", \"C\"),\n",
       "  nfolds = 10,\n",
       "  foldid = NULL,\n",
       "  alignment = c(\"lambda\", \"fraction\"),\n",
       "  grouped = TRUE,\n",
       "  keep = FALSE,\n",
       "  parallel = FALSE,\n",
       "  gamma = c(0, 0.25, 0.5, 0.75, 1),\n",
       "  relax = FALSE,\n",
       "  trace.it = 0,\n",
       "  ...\n",
       ")\n",
       "\\end{verbatim}\n",
       "\\end{Usage}\n",
       "%\n",
       "\\begin{Arguments}\n",
       "\\begin{ldescription}\n",
       "\\item[\\code{x}] \\code{x} matrix as in \\code{glmnet}.\n",
       "\n",
       "\\item[\\code{y}] response \\code{y} as in \\code{glmnet}.\n",
       "\n",
       "\\item[\\code{weights}] Observation weights; defaults to 1 per observation\n",
       "\n",
       "\\item[\\code{offset}] Offset vector (matrix) as in \\code{glmnet}\n",
       "\n",
       "\\item[\\code{lambda}] Optional user-supplied lambda sequence; default is\n",
       "\\code{NULL}, and \\code{glmnet} chooses its own sequence. Note that this is done\n",
       "for the full model (master sequence), and separately for each fold.\n",
       "The fits are then alligned using the master sequence (see the \\code{allignment}\n",
       "argument for additional details). Adapting \\code{lambda} for each fold\n",
       "leads to better convergence. When \\code{lambda} is supplied, the same sequence\n",
       "is used everywhere, but in some GLMs can lead to convergence issues.\n",
       "\n",
       "\\item[\\code{type.measure}] loss to use for cross-validation. Currently five\n",
       "options, not all available for all models. The default is\n",
       "\\code{type.measure=\"deviance\"}, which uses squared-error for gaussian models\n",
       "(a.k.a \\code{type.measure=\"mse\"} there), deviance for logistic and poisson\n",
       "regression, and partial-likelihood for the Cox model.\n",
       "\\code{type.measure=\"class\"} applies to binomial and multinomial logistic\n",
       "regression only, and gives misclassification error.\n",
       "\\code{type.measure=\"auc\"} is for two-class logistic regression only, and\n",
       "gives area under the ROC curve. \\code{type.measure=\"mse\"} or\n",
       "\\code{type.measure=\"mae\"} (mean absolute error) can be used by all models\n",
       "except the \\code{\"cox\"}; they measure the deviation from the fitted mean to\n",
       "the response.\n",
       "\\code{type.measure=\"C\"} is Harrel's concordance measure, only available for \\code{cox} models.\n",
       "\n",
       "\\item[\\code{nfolds}] number of folds - default is 10. Although \\code{nfolds} can be\n",
       "as large as the sample size (leave-one-out CV), it is not recommended for\n",
       "large datasets. Smallest value allowable is \\code{nfolds=3}\n",
       "\n",
       "\\item[\\code{foldid}] an optional vector of values between 1 and \\code{nfolds}\n",
       "identifying what fold each observation is in. If supplied, \\code{nfolds} can\n",
       "be missing.\n",
       "\n",
       "\\item[\\code{alignment}] This is an experimental argument, designed to fix the\n",
       "problems users were having with CV, with possible values \\code{\"lambda\"}\n",
       "(the default) else \\code{\"fraction\"}. With \\code{\"lambda\"} the \\code{lambda}\n",
       "values from the master fit (on all the data) are used to line up the\n",
       "predictions from each of the folds. In some cases this can give strange\n",
       "values, since the effective \\code{lambda} values in each fold could be quite\n",
       "different. With \\code{\"fraction\"} we line up the predictions in each fold\n",
       "according to the fraction of progress along the regularization. If in the\n",
       "call a \\code{lambda} argument is also provided, \\code{alignment=\"fraction\"}\n",
       "is ignored (with a warning).\n",
       "\n",
       "\\item[\\code{grouped}] This is an experimental argument, with default \\code{TRUE},\n",
       "and can be ignored by most users. For all models except the \\code{\"cox\"},\n",
       "this refers to computing \\code{nfolds} separate statistics, and then using\n",
       "their mean and estimated standard error to describe the CV curve. If\n",
       "\\code{grouped=FALSE}, an error matrix is built up at the observation level\n",
       "from the predictions from the \\code{nfolds} fits, and then summarized (does\n",
       "not apply to \\code{type.measure=\"auc\"}). For the \\code{\"cox\"} family,\n",
       "\\code{grouped=TRUE} obtains the CV partial likelihood for the Kth fold by\n",
       "\\emph{subtraction}; by subtracting the log partial likelihood evaluated on\n",
       "the full dataset from that evaluated on the on the (K-1)/K dataset. This\n",
       "makes more efficient use of risk sets. With \\code{grouped=FALSE} the log\n",
       "partial likelihood is computed only on the Kth fold\n",
       "\n",
       "\\item[\\code{keep}] If \\code{keep=TRUE}, a \\emph{prevalidated} array is returned\n",
       "containing fitted values for each observation and each value of\n",
       "\\code{lambda}. This means these fits are computed with this observation and\n",
       "the rest of its fold omitted. The \\code{foldid} vector is also returned.\n",
       "Default is keep=FALSE.  If \\code{relax=TRUE}, then a list of such arrays is\n",
       "returned, one for each value of 'gamma'. Note: if the value 'gamma=1' is\n",
       "omitted, this case is included in the list since it corresponds to the\n",
       "original 'glmnet' fit.\n",
       "\n",
       "\\item[\\code{parallel}] If \\code{TRUE}, use parallel \\code{foreach} to fit each\n",
       "fold.  Must register parallel before hand, such as \\code{doMC} or others.\n",
       "See the example below.\n",
       "\n",
       "\\item[\\code{gamma}] The values of the parameter for mixing the relaxed fit with the\n",
       "regularized fit, between 0 and 1; default is \\code{gamma = c(0, 0.25, 0.5,\n",
       "0.75, 1)}\n",
       "\n",
       "\\item[\\code{relax}] If \\code{TRUE}, then CV is done with respect to the mixing\n",
       "parameter \\code{gamma} as well as \\code{lambda}. Default is\n",
       "\\code{relax=FALSE}\n",
       "\n",
       "\\item[\\code{trace.it}] If \\code{trace.it=1}, then progress bars are displayed;\n",
       "useful for big models that take a long time to fit. Limited tracing if\n",
       "\\code{parallel=TRUE}\n",
       "\n",
       "\\item[\\code{...}] Other arguments that can be passed to \\code{glmnet}\n",
       "\\end{ldescription}\n",
       "\\end{Arguments}\n",
       "%\n",
       "\\begin{Details}\n",
       "The function runs \\code{glmnet} \\code{nfolds}+1 times; the first to get the\n",
       "\\code{lambda} sequence, and then the remainder to compute the fit with each\n",
       "of the folds omitted. The error is accumulated, and the average error and\n",
       "standard deviation over the folds is computed.  Note that \\code{cv.glmnet}\n",
       "does NOT search for values for \\code{alpha}. A specific value should be\n",
       "supplied, else \\code{alpha=1} is assumed by default. If users would like to\n",
       "cross-validate \\code{alpha} as well, they should call \\code{cv.glmnet} with\n",
       "a pre-computed vector \\code{foldid}, and then use this same fold vector in\n",
       "separate calls to \\code{cv.glmnet} with different values of \\code{alpha}.\n",
       "Note also that the results of \\code{cv.glmnet} are random, since the folds\n",
       "are selected at random. Users can reduce this randomness by running\n",
       "\\code{cv.glmnet} many times, and averaging the error curves.\n",
       "\n",
       "If \\code{relax=TRUE} then the values of \\code{gamma} are used to mix the\n",
       "fits. If \\eqn{\\eta}{} is the fit for lasso/elastic net, and \\eqn{\\eta_R}{} is\n",
       "the relaxed fit (with unpenalized coefficients), then a relaxed fit mixed by\n",
       "\\eqn{\\gamma}{} is \\deqn{\\eta(\\gamma)=(1-\\gamma)\\eta_R+\\gamma\\eta.}{} There is\n",
       "practically no extra cost for having a lot of values for \\code{gamma}.\n",
       "However, 5 seems sufficient for most purposes. CV then selects both\n",
       "\\code{gamma} and \\code{lambda}.\n",
       "\\end{Details}\n",
       "%\n",
       "\\begin{Value}\n",
       "an object of class \\code{\"cv.glmnet\"} is returned, which is a list\n",
       "with the ingredients of the cross-validation fit.  If the object was created\n",
       "with \\code{relax=TRUE} then this class has a prefix class of\n",
       "\\code{\"cv.relaxed\"}.  \\begin{ldescription}\n",
       "\\item[\\code{lambda}] the values of \\code{lambda} used in the\n",
       "fits.\\item[\\code{cvm}] The mean cross-validated error - a vector of length\n",
       "\\code{length(lambda)}.\\item[\\code{cvsd}] estimate of standard error of\n",
       "\\code{cvm}.\\item[\\code{cvup}] upper curve = \\code{cvm+cvsd}.\\item[\\code{cvlo}] lower\n",
       "curve = \\code{cvm-cvsd}.\\item[\\code{nzero}] number of non-zero coefficients at\n",
       "each \\code{lambda}.\\item[\\code{name}] a text string indicating type of measure\n",
       "(for plotting purposes).\\item[\\code{glmnet.fit}] a fitted glmnet object for the\n",
       "full data.\\item[\\code{lambda.min}] value of \\code{lambda} that gives minimum\n",
       "\\code{cvm}.\\item[\\code{lambda.1se}] largest value of \\code{lambda} such that\n",
       "error is within 1 standard error of the minimum.\\item[\\code{fit.preval}] if\n",
       "\\code{keep=TRUE}, this is the array of prevalidated fits. Some entries can\n",
       "be \\code{NA}, if that and subsequent values of \\code{lambda} are not reached\n",
       "for that fold\\item[\\code{foldid}] if \\code{keep=TRUE}, the fold assignments used\n",
       "\\item[\\code{index}] a one column matrix with the indices of \\code{lambda.min} and \\code{lambda.1se} in the sequence of coefficients, fits etc.\n",
       "\\item[\\code{relaxed}] if \\code{relax=TRUE}, this additional item has the CV info\n",
       "for each of the mixed fits. In particular it also selects \\code{lambda,\n",
       "gamma} pairs corresponding to the 1se rule, as well as the minimum error. It also has a component \\code{index}, a two-column matrix  which contains the \\code{lambda} and \\code{gamma} indices corresponding to the \"min\" and \"1se\" solutions.\n",
       "\\end{ldescription}\n",
       "\\end{Value}\n",
       "%\n",
       "\\begin{Author}\n",
       "Jerome Friedman, Trevor Hastie and Rob Tibshirani\\\\{} Noah Simon\n",
       "helped develop the 'coxnet' function.\\\\{} Jeffrey Wong and B. Narasimhan\n",
       "helped with the parallel option\\\\{} Maintainer: Trevor Hastie\n",
       "\\email{hastie@stanford.edu}\n",
       "\\end{Author}\n",
       "%\n",
       "\\begin{References}\n",
       "Friedman, J., Hastie, T. and Tibshirani, R. (2008)\n",
       "\\emph{Regularization Paths for Generalized Linear Models via Coordinate\n",
       "Descent (2010), Journal of Statistical Software, Vol. 33(1), 1-22},\n",
       "\\Rhref{https://doi.org/10.18637/jss.v033.i01}{doi:10.18637\\slash{}jss.v033.i01}.\\\\{}\n",
       "Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2011)\n",
       "\\emph{Regularization Paths for Cox's Proportional\n",
       "Hazards Model via Coordinate Descent, Journal of Statistical Software, Vol.\n",
       "39(5), 1-13},\n",
       "\\Rhref{https://doi.org/10.18637/jss.v039.i05}{doi:10.18637\\slash{}jss.v039.i05}.\n",
       "\\end{References}\n",
       "%\n",
       "\\begin{SeeAlso}\n",
       "\\code{glmnet} and \\code{plot}, \\code{predict}, and \\code{coef}\n",
       "methods for \\code{\"cv.glmnet\"} and \\code{\"cv.relaxed\"} objects.\n",
       "\\end{SeeAlso}\n",
       "%\n",
       "\\begin{Examples}\n",
       "\\begin{ExampleCode}\n",
       "\n",
       "set.seed(1010)\n",
       "n = 1000\n",
       "p = 100\n",
       "nzc = trunc(p/10)\n",
       "x = matrix(rnorm(n * p), n, p)\n",
       "beta = rnorm(nzc)\n",
       "fx = x[, seq(nzc)] %*% beta\n",
       "eps = rnorm(n) * 5\n",
       "y = drop(fx + eps)\n",
       "px = exp(fx)\n",
       "px = px/(1 + px)\n",
       "ly = rbinom(n = length(px), prob = px, size = 1)\n",
       "set.seed(1011)\n",
       "cvob1 = cv.glmnet(x, y)\n",
       "plot(cvob1)\n",
       "coef(cvob1)\n",
       "predict(cvob1, newx = x[1:5, ], s = \"lambda.min\")\n",
       "title(\"Gaussian Family\", line = 2.5)\n",
       "set.seed(1011)\n",
       "cvob1a = cv.glmnet(x, y, type.measure = \"mae\")\n",
       "plot(cvob1a)\n",
       "title(\"Gaussian Family\", line = 2.5)\n",
       "set.seed(1011)\n",
       "par(mfrow = c(2, 2), mar = c(4.5, 4.5, 4, 1))\n",
       "cvob2 = cv.glmnet(x, ly, family = \"binomial\")\n",
       "plot(cvob2)\n",
       "title(\"Binomial Family\", line = 2.5)\n",
       "frame()\n",
       "set.seed(1011)\n",
       "cvob3 = cv.glmnet(x, ly, family = \"binomial\", type.measure = \"class\")\n",
       "plot(cvob3)\n",
       "title(\"Binomial Family\", line = 2.5)\n",
       "## Not run: \n",
       "cvob1r = cv.glmnet(x, y, relax = TRUE)\n",
       "plot(cvob1r)\n",
       "predict(cvob1r, newx = x[, 1:5])\n",
       "set.seed(1011)\n",
       "cvob3a = cv.glmnet(x, ly, family = \"binomial\", type.measure = \"auc\")\n",
       "plot(cvob3a)\n",
       "title(\"Binomial Family\", line = 2.5)\n",
       "set.seed(1011)\n",
       "mu = exp(fx/10)\n",
       "y = rpois(n, mu)\n",
       "cvob4 = cv.glmnet(x, y, family = \"poisson\")\n",
       "plot(cvob4)\n",
       "title(\"Poisson Family\", line = 2.5)\n",
       "\n",
       "# Multinomial\n",
       "n = 500\n",
       "p = 30\n",
       "nzc = trunc(p/10)\n",
       "x = matrix(rnorm(n * p), n, p)\n",
       "beta3 = matrix(rnorm(30), 10, 3)\n",
       "beta3 = rbind(beta3, matrix(0, p - 10, 3))\n",
       "f3 = x %*% beta3\n",
       "p3 = exp(f3)\n",
       "p3 = p3/apply(p3, 1, sum)\n",
       "g3 = glmnet:::rmult(p3)\n",
       "set.seed(10101)\n",
       "cvfit = cv.glmnet(x, g3, family = \"multinomial\")\n",
       "plot(cvfit)\n",
       "title(\"Multinomial Family\", line = 2.5)\n",
       "# Cox\n",
       "beta = rnorm(nzc)\n",
       "fx = x[, seq(nzc)] %*% beta/3\n",
       "hx = exp(fx)\n",
       "ty = rexp(n, hx)\n",
       "tcens = rbinom(n = n, prob = 0.3, size = 1)  # censoring indicator\n",
       "y = cbind(time = ty, status = 1 - tcens)  # y=Surv(ty,1-tcens) with library(survival)\n",
       "foldid = sample(rep(seq(10), length = n))\n",
       "fit1_cv = cv.glmnet(x, y, family = \"cox\", foldid = foldid)\n",
       "plot(fit1_cv)\n",
       "title(\"Cox Family\", line = 2.5)\n",
       "# Parallel\n",
       "require(doMC)\n",
       "registerDoMC(cores = 4)\n",
       "x = matrix(rnorm(1e+05 * 100), 1e+05, 100)\n",
       "y = rnorm(1e+05)\n",
       "system.time(cv.glmnet(x, y))\n",
       "system.time(cv.glmnet(x, y, parallel = TRUE))\n",
       "\n",
       "## End(Not run)\n",
       "\n",
       "\\end{ExampleCode}\n",
       "\\end{Examples}"
      ],
      "text/plain": [
       "cv.glmnet                package:glmnet                R Documentation\n",
       "\n",
       "_\bC_\br_\bo_\bs_\bs-_\bv_\ba_\bl_\bi_\bd_\ba_\bt_\bi_\bo_\bn _\bf_\bo_\br _\bg_\bl_\bm_\bn_\be_\bt\n",
       "\n",
       "_\bD_\be_\bs_\bc_\br_\bi_\bp_\bt_\bi_\bo_\bn:\n",
       "\n",
       "     Does k-fold cross-validation for glmnet, produces a plot, and\n",
       "     returns a value for ‘lambda’ (and ‘gamma’ if ‘relax=TRUE’)\n",
       "\n",
       "_\bU_\bs_\ba_\bg_\be:\n",
       "\n",
       "     cv.glmnet(\n",
       "       x,\n",
       "       y,\n",
       "       weights = NULL,\n",
       "       offset = NULL,\n",
       "       lambda = NULL,\n",
       "       type.measure = c(\"default\", \"mse\", \"deviance\", \"class\", \"auc\", \"mae\", \"C\"),\n",
       "       nfolds = 10,\n",
       "       foldid = NULL,\n",
       "       alignment = c(\"lambda\", \"fraction\"),\n",
       "       grouped = TRUE,\n",
       "       keep = FALSE,\n",
       "       parallel = FALSE,\n",
       "       gamma = c(0, 0.25, 0.5, 0.75, 1),\n",
       "       relax = FALSE,\n",
       "       trace.it = 0,\n",
       "       ...\n",
       "     )\n",
       "     \n",
       "_\bA_\br_\bg_\bu_\bm_\be_\bn_\bt_\bs:\n",
       "\n",
       "       x: ‘x’ matrix as in ‘glmnet’.\n",
       "\n",
       "       y: response ‘y’ as in ‘glmnet’.\n",
       "\n",
       " weights: Observation weights; defaults to 1 per observation\n",
       "\n",
       "  offset: Offset vector (matrix) as in ‘glmnet’\n",
       "\n",
       "  lambda: Optional user-supplied lambda sequence; default is ‘NULL’,\n",
       "          and ‘glmnet’ chooses its own sequence. Note that this is done\n",
       "          for the full model (master sequence), and separately for each\n",
       "          fold. The fits are then alligned using the master sequence\n",
       "          (see the ‘allignment’ argument for additional details).\n",
       "          Adapting ‘lambda’ for each fold leads to better convergence.\n",
       "          When ‘lambda’ is supplied, the same sequence is used\n",
       "          everywhere, but in some GLMs can lead to convergence issues.\n",
       "\n",
       "type.measure: loss to use for cross-validation. Currently five options,\n",
       "          not all available for all models. The default is\n",
       "          ‘type.measure=\"deviance\"’, which uses squared-error for\n",
       "          gaussian models (a.k.a ‘type.measure=\"mse\"’ there), deviance\n",
       "          for logistic and poisson regression, and partial-likelihood\n",
       "          for the Cox model. ‘type.measure=\"class\"’ applies to binomial\n",
       "          and multinomial logistic regression only, and gives\n",
       "          misclassification error. ‘type.measure=\"auc\"’ is for\n",
       "          two-class logistic regression only, and gives area under the\n",
       "          ROC curve. ‘type.measure=\"mse\"’ or ‘type.measure=\"mae\"’ (mean\n",
       "          absolute error) can be used by all models except the ‘\"cox\"’;\n",
       "          they measure the deviation from the fitted mean to the\n",
       "          response. ‘type.measure=\"C\"’ is Harrel's concordance measure,\n",
       "          only available for ‘cox’ models.\n",
       "\n",
       "  nfolds: number of folds - default is 10. Although ‘nfolds’ can be as\n",
       "          large as the sample size (leave-one-out CV), it is not\n",
       "          recommended for large datasets. Smallest value allowable is\n",
       "          ‘nfolds=3’\n",
       "\n",
       "  foldid: an optional vector of values between 1 and ‘nfolds’\n",
       "          identifying what fold each observation is in. If supplied,\n",
       "          ‘nfolds’ can be missing.\n",
       "\n",
       "alignment: This is an experimental argument, designed to fix the\n",
       "          problems users were having with CV, with possible values\n",
       "          ‘\"lambda\"’ (the default) else ‘\"fraction\"’. With ‘\"lambda\"’\n",
       "          the ‘lambda’ values from the master fit (on all the data) are\n",
       "          used to line up the predictions from each of the folds. In\n",
       "          some cases this can give strange values, since the effective\n",
       "          ‘lambda’ values in each fold could be quite different. With\n",
       "          ‘\"fraction\"’ we line up the predictions in each fold\n",
       "          according to the fraction of progress along the\n",
       "          regularization. If in the call a ‘lambda’ argument is also\n",
       "          provided, ‘alignment=\"fraction\"’ is ignored (with a warning).\n",
       "\n",
       " grouped: This is an experimental argument, with default ‘TRUE’, and\n",
       "          can be ignored by most users. For all models except the\n",
       "          ‘\"cox\"’, this refers to computing ‘nfolds’ separate\n",
       "          statistics, and then using their mean and estimated standard\n",
       "          error to describe the CV curve. If ‘grouped=FALSE’, an error\n",
       "          matrix is built up at the observation level from the\n",
       "          predictions from the ‘nfolds’ fits, and then summarized (does\n",
       "          not apply to ‘type.measure=\"auc\"’). For the ‘\"cox\"’ family,\n",
       "          ‘grouped=TRUE’ obtains the CV partial likelihood for the Kth\n",
       "          fold by _subtraction_; by subtracting the log partial\n",
       "          likelihood evaluated on the full dataset from that evaluated\n",
       "          on the on the (K-1)/K dataset. This makes more efficient use\n",
       "          of risk sets. With ‘grouped=FALSE’ the log partial likelihood\n",
       "          is computed only on the Kth fold\n",
       "\n",
       "    keep: If ‘keep=TRUE’, a _prevalidated_ array is returned containing\n",
       "          fitted values for each observation and each value of\n",
       "          ‘lambda’. This means these fits are computed with this\n",
       "          observation and the rest of its fold omitted. The ‘foldid’\n",
       "          vector is also returned. Default is keep=FALSE.  If\n",
       "          ‘relax=TRUE’, then a list of such arrays is returned, one for\n",
       "          each value of 'gamma'. Note: if the value 'gamma=1' is\n",
       "          omitted, this case is included in the list since it\n",
       "          corresponds to the original 'glmnet' fit.\n",
       "\n",
       "parallel: If ‘TRUE’, use parallel ‘foreach’ to fit each fold.  Must\n",
       "          register parallel before hand, such as ‘doMC’ or others. See\n",
       "          the example below.\n",
       "\n",
       "   gamma: The values of the parameter for mixing the relaxed fit with\n",
       "          the regularized fit, between 0 and 1; default is ‘gamma =\n",
       "          c(0, 0.25, 0.5, 0.75, 1)’\n",
       "\n",
       "   relax: If ‘TRUE’, then CV is done with respect to the mixing\n",
       "          parameter ‘gamma’ as well as ‘lambda’. Default is\n",
       "          ‘relax=FALSE’\n",
       "\n",
       "trace.it: If ‘trace.it=1’, then progress bars are displayed; useful for\n",
       "          big models that take a long time to fit. Limited tracing if\n",
       "          ‘parallel=TRUE’\n",
       "\n",
       "     ...: Other arguments that can be passed to ‘glmnet’\n",
       "\n",
       "_\bD_\be_\bt_\ba_\bi_\bl_\bs:\n",
       "\n",
       "     The function runs ‘glmnet’ ‘nfolds’+1 times; the first to get the\n",
       "     ‘lambda’ sequence, and then the remainder to compute the fit with\n",
       "     each of the folds omitted. The error is accumulated, and the\n",
       "     average error and standard deviation over the folds is computed.\n",
       "     Note that ‘cv.glmnet’ does NOT search for values for ‘alpha’. A\n",
       "     specific value should be supplied, else ‘alpha=1’ is assumed by\n",
       "     default. If users would like to cross-validate ‘alpha’ as well,\n",
       "     they should call ‘cv.glmnet’ with a pre-computed vector ‘foldid’,\n",
       "     and then use this same fold vector in separate calls to\n",
       "     ‘cv.glmnet’ with different values of ‘alpha’. Note also that the\n",
       "     results of ‘cv.glmnet’ are random, since the folds are selected at\n",
       "     random. Users can reduce this randomness by running ‘cv.glmnet’\n",
       "     many times, and averaging the error curves.\n",
       "\n",
       "     If ‘relax=TRUE’ then the values of ‘gamma’ are used to mix the\n",
       "     fits. If eta is the fit for lasso/elastic net, and eta_R is the\n",
       "     relaxed fit (with unpenalized coefficients), then a relaxed fit\n",
       "     mixed by gamma is\n",
       "\n",
       "                     eta(gamma)=(1-gamma)eta_R+gammaeta.                \n",
       "     \n",
       "     There is practically no extra cost for having a lot of values for\n",
       "     ‘gamma’. However, 5 seems sufficient for most purposes. CV then\n",
       "     selects both ‘gamma’ and ‘lambda’.\n",
       "\n",
       "_\bV_\ba_\bl_\bu_\be:\n",
       "\n",
       "     an object of class ‘\"cv.glmnet\"’ is returned, which is a list with\n",
       "     the ingredients of the cross-validation fit.  If the object was\n",
       "     created with ‘relax=TRUE’ then this class has a prefix class of\n",
       "     ‘\"cv.relaxed\"’.\n",
       "\n",
       "  lambda: the values of ‘lambda’ used in the fits.\n",
       "\n",
       "     cvm: The mean cross-validated error - a vector of length\n",
       "          ‘length(lambda)’.\n",
       "\n",
       "    cvsd: estimate of standard error of ‘cvm’.\n",
       "\n",
       "    cvup: upper curve = ‘cvm+cvsd’.\n",
       "\n",
       "    cvlo: lower curve = ‘cvm-cvsd’.\n",
       "\n",
       "   nzero: number of non-zero coefficients at each ‘lambda’.\n",
       "\n",
       "    name: a text string indicating type of measure (for plotting\n",
       "          purposes).\n",
       "\n",
       "glmnet.fit: a fitted glmnet object for the full data.\n",
       "\n",
       "lambda.min: value of ‘lambda’ that gives minimum ‘cvm’.\n",
       "\n",
       "lambda.1se: largest value of ‘lambda’ such that error is within 1\n",
       "          standard error of the minimum.\n",
       "\n",
       "fit.preval: if ‘keep=TRUE’, this is the array of prevalidated fits.\n",
       "          Some entries can be ‘NA’, if that and subsequent values of\n",
       "          ‘lambda’ are not reached for that fold\n",
       "\n",
       "  foldid: if ‘keep=TRUE’, the fold assignments used\n",
       "\n",
       "   index: a one column matrix with the indices of ‘lambda.min’ and\n",
       "          ‘lambda.1se’ in the sequence of coefficients, fits etc.\n",
       "\n",
       " relaxed: if ‘relax=TRUE’, this additional item has the CV info for\n",
       "          each of the mixed fits. In particular it also selects\n",
       "          ‘lambda, gamma’ pairs corresponding to the 1se rule, as well\n",
       "          as the minimum error. It also has a component ‘index’, a\n",
       "          two-column matrix which contains the ‘lambda’ and ‘gamma’\n",
       "          indices corresponding to the \"min\" and \"1se\" solutions.\n",
       "\n",
       "_\bA_\bu_\bt_\bh_\bo_\br(_\bs):\n",
       "\n",
       "     Jerome Friedman, Trevor Hastie and Rob Tibshirani\n",
       "     Noah Simon helped develop the 'coxnet' function.\n",
       "     Jeffrey Wong and B. Narasimhan helped with the parallel option\n",
       "     Maintainer: Trevor Hastie <mailto:hastie@stanford.edu>\n",
       "\n",
       "_\bR_\be_\bf_\be_\br_\be_\bn_\bc_\be_\bs:\n",
       "\n",
       "     Friedman, J., Hastie, T. and Tibshirani, R. (2008) _Regularization\n",
       "     Paths for Generalized Linear Models via Coordinate Descent (2010),\n",
       "     Journal of Statistical Software, Vol. 33(1), 1-22_,\n",
       "     doi:10.18637/jss.v033.i01 <https://doi.org/10.18637/jss.v033.i01>.\n",
       "     Simon, N., Friedman, J., Hastie, T. and Tibshirani, R. (2011)\n",
       "     _Regularization Paths for Cox's Proportional Hazards Model via\n",
       "     Coordinate Descent, Journal of Statistical Software, Vol. 39(5),\n",
       "     1-13_, doi:10.18637/jss.v039.i05\n",
       "     <https://doi.org/10.18637/jss.v039.i05>.\n",
       "\n",
       "_\bS_\be_\be _\bA_\bl_\bs_\bo:\n",
       "\n",
       "     ‘glmnet’ and ‘plot’, ‘predict’, and ‘coef’ methods for\n",
       "     ‘\"cv.glmnet\"’ and ‘\"cv.relaxed\"’ objects.\n",
       "\n",
       "_\bE_\bx_\ba_\bm_\bp_\bl_\be_\bs:\n",
       "\n",
       "     set.seed(1010)\n",
       "     n = 1000\n",
       "     p = 100\n",
       "     nzc = trunc(p/10)\n",
       "     x = matrix(rnorm(n * p), n, p)\n",
       "     beta = rnorm(nzc)\n",
       "     fx = x[, seq(nzc)] %*% beta\n",
       "     eps = rnorm(n) * 5\n",
       "     y = drop(fx + eps)\n",
       "     px = exp(fx)\n",
       "     px = px/(1 + px)\n",
       "     ly = rbinom(n = length(px), prob = px, size = 1)\n",
       "     set.seed(1011)\n",
       "     cvob1 = cv.glmnet(x, y)\n",
       "     plot(cvob1)\n",
       "     coef(cvob1)\n",
       "     predict(cvob1, newx = x[1:5, ], s = \"lambda.min\")\n",
       "     title(\"Gaussian Family\", line = 2.5)\n",
       "     set.seed(1011)\n",
       "     cvob1a = cv.glmnet(x, y, type.measure = \"mae\")\n",
       "     plot(cvob1a)\n",
       "     title(\"Gaussian Family\", line = 2.5)\n",
       "     set.seed(1011)\n",
       "     par(mfrow = c(2, 2), mar = c(4.5, 4.5, 4, 1))\n",
       "     cvob2 = cv.glmnet(x, ly, family = \"binomial\")\n",
       "     plot(cvob2)\n",
       "     title(\"Binomial Family\", line = 2.5)\n",
       "     frame()\n",
       "     set.seed(1011)\n",
       "     cvob3 = cv.glmnet(x, ly, family = \"binomial\", type.measure = \"class\")\n",
       "     plot(cvob3)\n",
       "     title(\"Binomial Family\", line = 2.5)\n",
       "     ## Not run:\n",
       "     \n",
       "     cvob1r = cv.glmnet(x, y, relax = TRUE)\n",
       "     plot(cvob1r)\n",
       "     predict(cvob1r, newx = x[, 1:5])\n",
       "     set.seed(1011)\n",
       "     cvob3a = cv.glmnet(x, ly, family = \"binomial\", type.measure = \"auc\")\n",
       "     plot(cvob3a)\n",
       "     title(\"Binomial Family\", line = 2.5)\n",
       "     set.seed(1011)\n",
       "     mu = exp(fx/10)\n",
       "     y = rpois(n, mu)\n",
       "     cvob4 = cv.glmnet(x, y, family = \"poisson\")\n",
       "     plot(cvob4)\n",
       "     title(\"Poisson Family\", line = 2.5)\n",
       "     \n",
       "     # Multinomial\n",
       "     n = 500\n",
       "     p = 30\n",
       "     nzc = trunc(p/10)\n",
       "     x = matrix(rnorm(n * p), n, p)\n",
       "     beta3 = matrix(rnorm(30), 10, 3)\n",
       "     beta3 = rbind(beta3, matrix(0, p - 10, 3))\n",
       "     f3 = x %*% beta3\n",
       "     p3 = exp(f3)\n",
       "     p3 = p3/apply(p3, 1, sum)\n",
       "     g3 = glmnet:::rmult(p3)\n",
       "     set.seed(10101)\n",
       "     cvfit = cv.glmnet(x, g3, family = \"multinomial\")\n",
       "     plot(cvfit)\n",
       "     title(\"Multinomial Family\", line = 2.5)\n",
       "     # Cox\n",
       "     beta = rnorm(nzc)\n",
       "     fx = x[, seq(nzc)] %*% beta/3\n",
       "     hx = exp(fx)\n",
       "     ty = rexp(n, hx)\n",
       "     tcens = rbinom(n = n, prob = 0.3, size = 1)  # censoring indicator\n",
       "     y = cbind(time = ty, status = 1 - tcens)  # y=Surv(ty,1-tcens) with library(survival)\n",
       "     foldid = sample(rep(seq(10), length = n))\n",
       "     fit1_cv = cv.glmnet(x, y, family = \"cox\", foldid = foldid)\n",
       "     plot(fit1_cv)\n",
       "     title(\"Cox Family\", line = 2.5)\n",
       "     # Parallel\n",
       "     require(doMC)\n",
       "     registerDoMC(cores = 4)\n",
       "     x = matrix(rnorm(1e+05 * 100), 1e+05, 100)\n",
       "     y = rnorm(1e+05)\n",
       "     system.time(cv.glmnet(x, y))\n",
       "     system.time(cv.glmnet(x, y, parallel = TRUE))\n",
       "     ## End(Not run)\n",
       "     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?cv.glmnet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
